{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Layer Perceptron via PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import (\n    UniformFloatHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom deepcave import Recorder, Objective\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nimport pytorch_lightning as pl\n\n\nclass MLP(pl.LightningModule):\n    def __init__(self, num_neurons=(64, 32), learning_rate=1e-4):\n        super().__init__()\n\n        self.data_dir = os.path.join(os.getcwd(), \"datasets\")\n        self.batch_size = 64\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        )\n\n        self.learning_rate = learning_rate\n        self.layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, num_neurons[0]),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(num_neurons[0], num_neurons[1]),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(num_neurons[1], self.num_classes),\n        )\n\n        self.accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.layers(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [20000, 40000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\n\ndef get_configspace(seed):\n    configspace = ConfigurationSpace(seed=seed)\n    num_neurons_layer1 = UniformIntegerHyperparameter(\n        name=\"num_neurons_layer1\", lower=5, upper=100\n    )\n    num_neurons_layer2 = UniformIntegerHyperparameter(\n        name=\"num_neurons_layer2\", lower=5, upper=100\n    )\n    learning_rate = UniformFloatHyperparameter(\n        name=\"learning_rate\", lower=0.0001, upper=0.1, log=True\n    )\n\n    configspace.add_hyperparameters(\n        [\n            num_neurons_layer1,\n            num_neurons_layer2,\n            learning_rate,\n        ]\n    )\n\n    return configspace\n\n\nif __name__ == \"__main__\":\n    # Define objectives\n    accuracy = Objective(\"accuracy\", lower=0, upper=1, optimize=\"upper\")\n    time = Objective(\"time\", lower=0, optimize=\"lower\")\n\n    # Define budgets\n    budgets = [1, 2, 3, 4, 5]\n\n    # Others\n    num_configs = 20\n    num_runs = 3\n    save_path = \"examples/record/logs/DeepCAVE/mlp_pytorch\"\n\n    for run_id in range(num_runs):\n        configspace = get_configspace(run_id)\n\n        with Recorder(\n            configspace, objectives=[accuracy, time], save_path=save_path\n        ) as r:\n            for config in configspace.sample_configuration(num_configs):\n                mlp = MLP(\n                    num_neurons=(\n                        config[\"num_neurons_layer1\"],\n                        config[\"num_neurons_layer2\"],\n                    ),\n                    learning_rate=config[\"learning_rate\"],\n                )\n\n                for budget in budgets:\n                    pl.seed_everything(run_id)\n                    r.start(config, budget, model=mlp)\n\n                    trainer = pl.Trainer(\n                        num_sanity_val_steps=0,  # No validation sanity\n                        auto_scale_batch_size=\"power\",\n                        gpus=0,\n                        deterministic=True,\n                        max_epochs=1,\n                    )\n                    trainer.fit(mlp)\n                    result = trainer.test(mlp)\n                    score = result[0][\"val_acc\"]\n\n                    r.end(costs=[score, None])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}